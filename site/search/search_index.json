{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Homepage","title":"Home"},{"location":"#homepage","text":"","title":"Homepage"},{"location":"api_module/__main__/","text":"main .py The __main__.py script serves as the entry point for the api module, facilitating the orchestration of API requests and data processing. Example Usage from api_client import ApiClient from api_data_processor import APIDataProcessor import json def main(): with open('api.json', 'r') as f: api_config = json.load(f) client = ApiClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Example query parameters query_params = { \"theme\": \"People\", \"polygon_wkb\": \"POLYGON((...))\" } # Fetch and process sensors data data_processor = APIDataProcessor() raw_sensors_data = client.get_sensors(query_params[\"theme\"], query_params[\"polygon_wkb\"]) sensors_df = data_processor.execute_sensors_request() # Process the data sensor_data = data_processor.execute_data_request() # Output the processed data print(sensor_data) if __name__ == \"__main__\": main()","title":"API Main"},{"location":"api_module/__main__/#mainpy","text":"The __main__.py script serves as the entry point for the api module, facilitating the orchestration of API requests and data processing.","title":"main.py"},{"location":"api_module/__main__/#example-usage","text":"from api_client import ApiClient from api_data_processor import APIDataProcessor import json def main(): with open('api.json', 'r') as f: api_config = json.load(f) client = ApiClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Example query parameters query_params = { \"theme\": \"People\", \"polygon_wkb\": \"POLYGON((...))\" } # Fetch and process sensors data data_processor = APIDataProcessor() raw_sensors_data = client.get_sensors(query_params[\"theme\"], query_params[\"polygon_wkb\"]) sensors_df = data_processor.execute_sensors_request() # Process the data sensor_data = data_processor.execute_data_request() # Output the processed data print(sensor_data) if __name__ == \"__main__\": main()","title":"Example Usage"},{"location":"api_module/api_client/","text":"Module: api_client.py The api_client.py script provides the core functionalities for making API requests and handling responses. It defines the APIClient class, which is used to interact with various endpoints of the API. Class: APIClient The APIClient class initialises with the base URL and token required for API access. It has several methods to interact with different endpoints defined in the api.json configuration file. Initialisation class APIClient: def __init__(self, base_url: str, token: str, endpoints: dict): self.base_url = base_url self.token = token self.endpoints = endpoints Methods get_sensors(theme: str, polygon_wkb: str) -> dict Fetches sensors based on the theme and polygon WKB (Well-Known Binary) format. Parameters : theme : The theme of the data (run APIClient.get_themes() for a full list). polygon_wkb : The polygon WKB defining the area of interest. Returns: JSON response containing sensor data. get_sensor_types() -> dict Fetches available sensor types. Returns: JSON response containing sensor types. get_themes() -> dict Fetches available themes. Returns: JSON response containing themes. get_variables(theme: str) -> dict Fetches variables based on the theme. Parameters: theme : The theme of the data (e.g., \u201cPeople\u201d). Returns: JSON response containing variables. get_data(sensor_name: str, starttime: str, endtime: str) -> dict Fetches data for a specific sensor within the given time range. Parameters: sensor_name : The name of the sensor. starttime : Start time for the data in YYYYMMDD format. endtime : End time for the data in YYYYMMDD format. Returns: JSON response containing sensor data. Example Usage from api_client import APIClient api_config = { \"base_url\": \"https://newcastle.urbanobservatory.ac.uk/api/v1.1/\", \"token\": \"your_api_token\", \"endpoints\": { # endpoint details } } client = APIClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Fetch sensors sensors = client.get_sensors(\"People\", \"POLYGON((...))\") # Fetch sensor types sensor_types = client.get_sensor_types() # Fetch themes themes = client.get_themes() # Fetch variables variables = client.get_variables(\"People\") # Fetch data data = client.get_data(\"sensor_name\", \"20220724\", \"20240724\")","title":"API Client"},{"location":"api_module/api_client/#module-api_clientpy","text":"The api_client.py script provides the core functionalities for making API requests and handling responses. It defines the APIClient class, which is used to interact with various endpoints of the API. Class: APIClient The APIClient class initialises with the base URL and token required for API access. It has several methods to interact with different endpoints defined in the api.json configuration file.","title":"Module: api_client.py"},{"location":"api_module/api_client/#initialisation","text":"class APIClient: def __init__(self, base_url: str, token: str, endpoints: dict): self.base_url = base_url self.token = token self.endpoints = endpoints","title":"Initialisation"},{"location":"api_module/api_client/#methods","text":"get_sensors(theme: str, polygon_wkb: str) -> dict Fetches sensors based on the theme and polygon WKB (Well-Known Binary) format. Parameters : theme : The theme of the data (run APIClient.get_themes() for a full list). polygon_wkb : The polygon WKB defining the area of interest. Returns: JSON response containing sensor data. get_sensor_types() -> dict Fetches available sensor types. Returns: JSON response containing sensor types. get_themes() -> dict Fetches available themes. Returns: JSON response containing themes. get_variables(theme: str) -> dict Fetches variables based on the theme. Parameters: theme : The theme of the data (e.g., \u201cPeople\u201d). Returns: JSON response containing variables. get_data(sensor_name: str, starttime: str, endtime: str) -> dict Fetches data for a specific sensor within the given time range. Parameters: sensor_name : The name of the sensor. starttime : Start time for the data in YYYYMMDD format. endtime : End time for the data in YYYYMMDD format. Returns: JSON response containing sensor data.","title":"Methods"},{"location":"api_module/api_client/#example-usage","text":"from api_client import APIClient api_config = { \"base_url\": \"https://newcastle.urbanobservatory.ac.uk/api/v1.1/\", \"token\": \"your_api_token\", \"endpoints\": { # endpoint details } } client = APIClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Fetch sensors sensors = client.get_sensors(\"People\", \"POLYGON((...))\") # Fetch sensor types sensor_types = client.get_sensor_types() # Fetch themes themes = client.get_themes() # Fetch variables variables = client.get_variables(\"People\") # Fetch data data = client.get_data(\"sensor_name\", \"20220724\", \"20240724\")","title":"Example Usage"},{"location":"api_module/api_data_processor/","text":"Module: api_data_processor.py The api_data_processor.py script handles the processing of data retrieved from the API. It defines the APIDataProcessor class, which is used to clean, preprocess, and manipulate the sensor data for further analysis. The APIDataProcessor requires a JSON config file with specified parameters. Class: APIDataProcessor The APIDataProcessor class initialises with configuration parameters and provides methods to execute various API requests and process the data. Initialisation class APIDataProcessor: def __init__(self): self.api_client = APIClient() self.config = get_api_config() self.window_size = get_window_size() self.min_df_length = get_min_df_length() self.ratio = get_min_df_length_to_window_size_ratio() Methods execute_sensors_request() -> pd.DataFrame Executes the API request to fetch sensor data and converts it to a DataFrame . Returns: DataFrame containing sensor data. execute_sensor_types_request() -> pd.DataFrame Executes the API request to fetch sensor types and converts it to a DataFrame . Returns: DataFrame containing sensor types. execute_themes_request() -> pd.DataFrame Executes the API request to fetch themes and converts it to a DataFrame . Returns: DataFrame containing themes. execute_variables_request() -> pd.DataFrame Executes the API request to fetch variables and converts it to a DataFrame . Returns: DataFrame containing variables. print_api_response_information(sensor_name: str, index: int, total_sensors: int) Prints information about the API response for a specific sensor. process_each_sensor(sensor_name: str, index: int, total_sensors: int) -> tuple Processes the data for a specific sensor. Parameters: sensor_name : The name of the sensor. index : The index of the sensor in the list. total_sensors : The total number of sensors. Returns: Tuple containing sensor name and DataFrame with sensor data. print_sensor_data_metrics(list_of_dataframes, series_of_sensor_names) Prints metrics about the processed sensor data. process_sensor_data_parallel(sensor_names: pd.Series) -> list Processes the data for sensors in parallel. Parameters: sensor_names : Series containing the names of the sensors. Returns: List of tuples containing sensor names and DataFrames with sensor data. execute_data_request() -> list Executes the data request and processes the data. Returns: List of tuples containing sensor names and DataFrames with sensor data. Example Usage from api_data_processor import APIDataProcessor data_processor = APIDataProcessor() # Execute sensor request and get the DataFrame sensors_df = data_processor.execute_sensors_request() # Execute sensor types request and get the DataFrame sensor_types_df = data_processor.execute_sensor_types_request() # Execute themes request and get the DataFrame themes_df = data_processor.execute_themes_request() # Execute variables request and get the DataFrame variables_df = data_processor.execute_variables_request() # Process sensor data sensor_data = data_processor.execute_data_request()","title":"API Data Processor"},{"location":"api_module/api_data_processor/#module-api_data_processorpy","text":"The api_data_processor.py script handles the processing of data retrieved from the API. It defines the APIDataProcessor class, which is used to clean, preprocess, and manipulate the sensor data for further analysis. The APIDataProcessor requires a JSON config file with specified parameters. Class: APIDataProcessor The APIDataProcessor class initialises with configuration parameters and provides methods to execute various API requests and process the data.","title":"Module: api_data_processor.py"},{"location":"api_module/api_data_processor/#initialisation","text":"class APIDataProcessor: def __init__(self): self.api_client = APIClient() self.config = get_api_config() self.window_size = get_window_size() self.min_df_length = get_min_df_length() self.ratio = get_min_df_length_to_window_size_ratio()","title":"Initialisation"},{"location":"api_module/api_data_processor/#methods","text":"execute_sensors_request() -> pd.DataFrame Executes the API request to fetch sensor data and converts it to a DataFrame . Returns: DataFrame containing sensor data. execute_sensor_types_request() -> pd.DataFrame Executes the API request to fetch sensor types and converts it to a DataFrame . Returns: DataFrame containing sensor types. execute_themes_request() -> pd.DataFrame Executes the API request to fetch themes and converts it to a DataFrame . Returns: DataFrame containing themes. execute_variables_request() -> pd.DataFrame Executes the API request to fetch variables and converts it to a DataFrame . Returns: DataFrame containing variables. print_api_response_information(sensor_name: str, index: int, total_sensors: int) Prints information about the API response for a specific sensor. process_each_sensor(sensor_name: str, index: int, total_sensors: int) -> tuple Processes the data for a specific sensor. Parameters: sensor_name : The name of the sensor. index : The index of the sensor in the list. total_sensors : The total number of sensors. Returns: Tuple containing sensor name and DataFrame with sensor data. print_sensor_data_metrics(list_of_dataframes, series_of_sensor_names) Prints metrics about the processed sensor data. process_sensor_data_parallel(sensor_names: pd.Series) -> list Processes the data for sensors in parallel. Parameters: sensor_names : Series containing the names of the sensors. Returns: List of tuples containing sensor names and DataFrames with sensor data. execute_data_request() -> list Executes the data request and processes the data. Returns: List of tuples containing sensor names and DataFrames with sensor data.","title":"Methods"},{"location":"api_module/api_data_processor/#example-usage","text":"from api_data_processor import APIDataProcessor data_processor = APIDataProcessor() # Execute sensor request and get the DataFrame sensors_df = data_processor.execute_sensors_request() # Execute sensor types request and get the DataFrame sensor_types_df = data_processor.execute_sensor_types_request() # Execute themes request and get the DataFrame themes_df = data_processor.execute_themes_request() # Execute variables request and get the DataFrame variables_df = data_processor.execute_variables_request() # Process sensor data sensor_data = data_processor.execute_data_request()","title":"Example Usage"},{"location":"api_module/overview/","text":"API Module Documentation This documentation provides an overview and usage details for the api module, which consists of functionalities to interact with the Newcastle Urban Observatory API. The module contains two main scripts: api_client.py and api_data_processor.py .","title":"API Module Overview"},{"location":"api_module/overview/#api-module-documentation","text":"This documentation provides an overview and usage details for the api module, which consists of functionalities to interact with the Newcastle Urban Observatory API. The module contains two main scripts: api_client.py and api_data_processor.py .","title":"API Module Documentation"},{"location":"pipeline_module/__main__/","text":"PHD Package Main File Documentation Overview The main file ( __main__.py ) serves as the entry point for the PHD package. It initialises and runs the entire data processing and modelling pipeline. File Location phd_package/__main__.py Code from .pipeline_generator import Pipeline if __name__ == \"__main__\": pipeline = Pipeline() print(f\"Running pipeline... {pipeline}\\n\") pipeline.run_pipeline() Functionality Import : The script imports the Pipeline class from the pipeline_generator module. Execution Check : The if __name__ == \"__main__\": condition ensures that the code only runs when the script is executed directly, not when it's imported as a module. Pipeline Initialisation : An instance of the Pipeline class is created. Status Print : A message is printed to indicate that the pipeline is running. Pipeline Execution : The run_pipeline() method of the Pipeline instance is called, which starts the entire data processing and modelling workflow. Usage To run the PHD package, execute the following command in your terminal: python -m phd_package.pipeline or with poetry : poetry run python -m phd_package.pipeline This command will: Load the necessary configurations Initialise the pipeline Execute all stages of data processing, from data acquisition to model testing Pipeline Stages The run_pipeline() method typically executes the following stages: Fetch data (currently executes the APIProcessor module - in future implementations this step will preferentially load data from a database if available) Preprocessing Feature engineering Data loading Model training Model testing Each of these stages is defined in separate modules within the package and is orchestrated by the Pipeline class. Customisation The behaviour of the pipeline can be customised by modifying the pipeline.json configuration file. This file allows you to set various parameters for each stage of the pipeline without changing the core code. Note Ensure that all dependencies are installed and the necessary data sources are accessible before running the package. Refer to the package's README file for setup instructions and requirements.","title":"Pipeline Main"},{"location":"pipeline_module/__main__/#phd-package-main-file-documentation","text":"","title":"PHD Package Main File Documentation"},{"location":"pipeline_module/__main__/#overview","text":"The main file ( __main__.py ) serves as the entry point for the PHD package. It initialises and runs the entire data processing and modelling pipeline.","title":"Overview"},{"location":"pipeline_module/__main__/#file-location","text":"phd_package/__main__.py","title":"File Location"},{"location":"pipeline_module/__main__/#code","text":"from .pipeline_generator import Pipeline if __name__ == \"__main__\": pipeline = Pipeline() print(f\"Running pipeline... {pipeline}\\n\") pipeline.run_pipeline()","title":"Code"},{"location":"pipeline_module/__main__/#functionality","text":"Import : The script imports the Pipeline class from the pipeline_generator module. Execution Check : The if __name__ == \"__main__\": condition ensures that the code only runs when the script is executed directly, not when it's imported as a module. Pipeline Initialisation : An instance of the Pipeline class is created. Status Print : A message is printed to indicate that the pipeline is running. Pipeline Execution : The run_pipeline() method of the Pipeline instance is called, which starts the entire data processing and modelling workflow.","title":"Functionality"},{"location":"pipeline_module/__main__/#usage","text":"To run the PHD package, execute the following command in your terminal: python -m phd_package.pipeline or with poetry : poetry run python -m phd_package.pipeline This command will: Load the necessary configurations Initialise the pipeline Execute all stages of data processing, from data acquisition to model testing","title":"Usage"},{"location":"pipeline_module/__main__/#pipeline-stages","text":"The run_pipeline() method typically executes the following stages: Fetch data (currently executes the APIProcessor module - in future implementations this step will preferentially load data from a database if available) Preprocessing Feature engineering Data loading Model training Model testing Each of these stages is defined in separate modules within the package and is orchestrated by the Pipeline class.","title":"Pipeline Stages"},{"location":"pipeline_module/__main__/#customisation","text":"The behaviour of the pipeline can be customised by modifying the pipeline.json configuration file. This file allows you to set various parameters for each stage of the pipeline without changing the core code.","title":"Customisation"},{"location":"pipeline_module/__main__/#note","text":"Ensure that all dependencies are installed and the necessary data sources are accessible before running the package. Refer to the package's README file for setup instructions and requirements.","title":"Note"},{"location":"pipeline_module/overview/","text":"PHD Package Pipeline Module Documentation Pipeline Module Overview The pipeline module is a core component of the PHD package, designed to handle the end-to-end process of time series data analysis and prediction. It encompasses several stages, from data loading to model training and evaluation. Key Components: Pipeline Generator : Orchestrates the entire data processing and modeling pipeline. Preprocessing : Includes functions for data cleaning and initial transformations. Feature Engineering : Provides methods for creating and transforming features. Dataloader : Handles the creation of PyTorch DataLoaders for model training. Model Training : Implements the training loop for neural network models. Model Testing : Evaluates the trained models on test data. Key Submodules 1. Pipeline Generator ( pipeline_generator.py ) The core submodule that orchestrates the entire data processing and modeling pipeline. It includes methods for each stage of the process, from data acquisition to model testing. 2. Preprocessing ( preprocessing.py ) Contains functions for data cleaning and initial transformations, such as: Removing directionality features Aggregating data on datetime Finding consecutive sequences Removing specified fields 3. Feature Engineering ( feature_engineering.py ) Provides methods for creating and transforming features, including: Scaling features Resampling frequency Adding term dates features Creating periodicity features Converting datetime to float 4. Dataloader ( dataloader.py ) Handles the creation of PyTorch DataLoaders for model training. Key functions include: sliding_windows : Generates sliding windows from time series data. create_dataloader : Prepares training, validation, and test dataloaders. add_model_to_dataloader : Adds the appropriate model to the dataloader pipeline. 5. Model Training ( train_model.py ) Implements the training loop for neural network models. The main function train_model handles: Model training over specified epochs Performance metric tracking Validation during training 6. Model Testing ( test_model.py ) Evaluates the trained models on test data. Key functions include: evaluate_model : Calculates performance metrics on a given dataset. test_model : Evaluates the trained model on the test dataset. Usage The pipeline module is designed to be used as part of the larger PHD package. It's typically initiated through the main function, which creates an instance of the Pipeline class and calls its run_pipeline() method. Each stage of the pipeline can also be used independently for more granular control over the data processing and modeling workflow. Pipeline Configuration (pipeline.json) The pipeline's behaviour is highly customisable through the pipeline.json configuration file. This file allows users to adjust various parameters and enable/disable specific processing steps without modifying the core code. Structure of pipeline.json The pipeline.json file is structured into several main sections: kwargs : Contains global parameters used across different stages of the pipeline. preprocessing : Defines the sequence and configuration of preprocessing steps. feature_engineering : Specifies the feature engineering steps to be applied. dataloader : Configures the data loading process. training : Sets up the model training parameters. testing : Configures the model testing process. Key Configuration Options Global Parameters ( kwargs ) aggregation_frequency_mins : Frequency for data aggregation (e.g., \"15min\"). datetime_column : Name of the datetime column (e.g., \"Timestamp\"). value_column : Name of the main value column (e.g., \"Value\"). window_size , horizon , stride : Parameters for sliding window creation. batch_size , shuffle , num_workers : DataLoader configuration. train_ratio , val_ratio : Data split ratios. model_type : Type of model to use (e.g., \" lstm \"). epochs , optimiser , lr : Training parameters. Stage-specific Configurations Each stage (preprocessing, feature_engineering, etc.) contains an array of steps, where each step is defined by: name : The full path to the function to be executed. execute_step : A boolean indicating whether to run this step (default is true). How It Works The pipeline reads the pipeline.json file at runtime. Global parameters from kwargs are used to configure various aspects of the pipeline. For each stage, the pipeline iterates through the specified steps: It checks the execute_step flag to determine whether to run the step. If true, it dynamically imports and executes the function specified in the name field. This configuration allows for easy addition, removal, or reordering of processing steps. Customisation Users can customise the pipeline by: Adjusting global parameters in the kwargs section. Enabling or disabling specific processing steps by modifying the execute_step flag. Changing the order of processing steps within each stage. Adding new processing steps by including new entries in the relevant stage arrays. This configuration-driven approach provides flexibility and allows users to tailor the pipeline to their specific needs without diving into the code base. Key Features Modular design allowing for easy customisation and extension Integration with PyTorch for deep learning model implementation Comprehensive approach to time series analysis, from data preprocessing to model evaluation Utilisation of MLflow for experiment tracking and metric logging Support for various types of models, including LSTM and Autoencoder architectures This pipeline module provides a robust framework for time series analysis and prediction, encapsulating best practices in data preprocessing, feature engineering, and model training for time series data.","title":"Pipeline Module Overview"},{"location":"pipeline_module/overview/#phd-package-pipeline-module-documentation","text":"","title":"PHD Package Pipeline Module Documentation"},{"location":"pipeline_module/overview/#pipeline-module-overview","text":"The pipeline module is a core component of the PHD package, designed to handle the end-to-end process of time series data analysis and prediction. It encompasses several stages, from data loading to model training and evaluation.","title":"Pipeline Module Overview"},{"location":"pipeline_module/overview/#key-components","text":"Pipeline Generator : Orchestrates the entire data processing and modeling pipeline. Preprocessing : Includes functions for data cleaning and initial transformations. Feature Engineering : Provides methods for creating and transforming features. Dataloader : Handles the creation of PyTorch DataLoaders for model training. Model Training : Implements the training loop for neural network models. Model Testing : Evaluates the trained models on test data.","title":"Key Components:"},{"location":"pipeline_module/overview/#key-submodules","text":"","title":"Key Submodules"},{"location":"pipeline_module/overview/#1-pipeline-generator-pipeline_generatorpy","text":"The core submodule that orchestrates the entire data processing and modeling pipeline. It includes methods for each stage of the process, from data acquisition to model testing.","title":"1. Pipeline Generator (pipeline_generator.py)"},{"location":"pipeline_module/overview/#2-preprocessing-preprocessingpy","text":"Contains functions for data cleaning and initial transformations, such as: Removing directionality features Aggregating data on datetime Finding consecutive sequences Removing specified fields","title":"2. Preprocessing (preprocessing.py)"},{"location":"pipeline_module/overview/#3-feature-engineering-feature_engineeringpy","text":"Provides methods for creating and transforming features, including: Scaling features Resampling frequency Adding term dates features Creating periodicity features Converting datetime to float","title":"3. Feature Engineering (feature_engineering.py)"},{"location":"pipeline_module/overview/#4-dataloader-dataloaderpy","text":"Handles the creation of PyTorch DataLoaders for model training. Key functions include: sliding_windows : Generates sliding windows from time series data. create_dataloader : Prepares training, validation, and test dataloaders. add_model_to_dataloader : Adds the appropriate model to the dataloader pipeline.","title":"4. Dataloader (dataloader.py)"},{"location":"pipeline_module/overview/#5-model-training-train_modelpy","text":"Implements the training loop for neural network models. The main function train_model handles: Model training over specified epochs Performance metric tracking Validation during training","title":"5. Model Training (train_model.py)"},{"location":"pipeline_module/overview/#6-model-testing-test_modelpy","text":"Evaluates the trained models on test data. Key functions include: evaluate_model : Calculates performance metrics on a given dataset. test_model : Evaluates the trained model on the test dataset.","title":"6. Model Testing (test_model.py)"},{"location":"pipeline_module/overview/#usage","text":"The pipeline module is designed to be used as part of the larger PHD package. It's typically initiated through the main function, which creates an instance of the Pipeline class and calls its run_pipeline() method. Each stage of the pipeline can also be used independently for more granular control over the data processing and modeling workflow.","title":"Usage"},{"location":"pipeline_module/overview/#pipeline-configuration-pipelinejson","text":"The pipeline's behaviour is highly customisable through the pipeline.json configuration file. This file allows users to adjust various parameters and enable/disable specific processing steps without modifying the core code. Structure of pipeline.json The pipeline.json file is structured into several main sections: kwargs : Contains global parameters used across different stages of the pipeline. preprocessing : Defines the sequence and configuration of preprocessing steps. feature_engineering : Specifies the feature engineering steps to be applied. dataloader : Configures the data loading process. training : Sets up the model training parameters. testing : Configures the model testing process.","title":"Pipeline Configuration (pipeline.json)"},{"location":"pipeline_module/overview/#key-configuration-options","text":"Global Parameters ( kwargs ) aggregation_frequency_mins : Frequency for data aggregation (e.g., \"15min\"). datetime_column : Name of the datetime column (e.g., \"Timestamp\"). value_column : Name of the main value column (e.g., \"Value\"). window_size , horizon , stride : Parameters for sliding window creation. batch_size , shuffle , num_workers : DataLoader configuration. train_ratio , val_ratio : Data split ratios. model_type : Type of model to use (e.g., \" lstm \"). epochs , optimiser , lr : Training parameters.","title":"Key Configuration Options"},{"location":"pipeline_module/overview/#stage-specific-configurations","text":"Each stage (preprocessing, feature_engineering, etc.) contains an array of steps, where each step is defined by: name : The full path to the function to be executed. execute_step : A boolean indicating whether to run this step (default is true).","title":"Stage-specific Configurations"},{"location":"pipeline_module/overview/#how-it-works","text":"The pipeline reads the pipeline.json file at runtime. Global parameters from kwargs are used to configure various aspects of the pipeline. For each stage, the pipeline iterates through the specified steps: It checks the execute_step flag to determine whether to run the step. If true, it dynamically imports and executes the function specified in the name field. This configuration allows for easy addition, removal, or reordering of processing steps.","title":"How It Works"},{"location":"pipeline_module/overview/#customisation","text":"Users can customise the pipeline by: Adjusting global parameters in the kwargs section. Enabling or disabling specific processing steps by modifying the execute_step flag. Changing the order of processing steps within each stage. Adding new processing steps by including new entries in the relevant stage arrays. This configuration-driven approach provides flexibility and allows users to tailor the pipeline to their specific needs without diving into the code base.","title":"Customisation"},{"location":"pipeline_module/overview/#key-features","text":"Modular design allowing for easy customisation and extension Integration with PyTorch for deep learning model implementation Comprehensive approach to time series analysis, from data preprocessing to model evaluation Utilisation of MLflow for experiment tracking and metric logging Support for various types of models, including LSTM and Autoencoder architectures This pipeline module provides a robust framework for time series analysis and prediction, encapsulating best practices in data preprocessing, feature engineering, and model training for time series data.","title":"Key Features"},{"location":"pipeline_module/pipeline/","text":"Pipeline Generator The Pipeline class is the core component of the data processing pipeline. It orchestrates the entire process from downloading sensor data to training and testing models. The module uses 'mlflow` for experiment tracking. Class: Pipeline Initialisation class Pipeline: def __init__(self, experiment_name: str = None): # Initialise the pipeline with an optional experiment name If no experiment_name is provided, a random string will be generated. The outputs from the model training and analysis are stored in the mlruns directory and can be viewed by calling mlflow ui in the command line. Methods read_or_download_sensors def read_or_download_sensors(self) -> SensorListItem: Downloads the list of sensors from the API or reads from local storage if available. Returns: A DataFrame containing the sensor list. read_or_download_data def read_or_download_data(self) -> RawDataItem: Reads raw sensor data from local storage if available or downloads it from the API. Returns: A list of tuples, each containing a sensor name and its raw data DataFrame. preprocess_data def preprocess_data(self, raw_dfs) -> PreprocessedItem: Preprocesses the raw data. Parameters: raw_dfs : List of tuples containing sensor names and raw data DataFrames. Returns: A list of tuples, each containing a sensor name and its preprocessed data DataFrame. apply_feature_engineering def apply_feature_engineering(self, preprocessed_dfs) -> EngineeredItem: Applies feature engineering to preprocessed data. Parameters: preprocessed_dfs : List of tuples containing sensor names and preprocessed data DataFrames. Returns: A list of tuples, each containing a sensor name and its feature-engineered data DataFrame. load_dataloader def load_dataloader(self, engineered_dfs) -> DataLoaderItem: Loads data into dataloaders . Parameters: engineered_dfs : List of tuples containing sensor names and feature-engineered data DataFrames. Returns: A list of tuples, each containing a sensor name, model, train loader, validation loader, and test loader. train_model def train_model(self, dataloaders_list) -> TrainedModelItem: Trains models using the prepared dataloaders . Parameters: dataloaders_list : List of tuples containing sensor names, models, and dataloaders . Returns: A list of tuples, each containing a sensor name, trained model, test loader, training metrics, and validation metrics. test_model def test_model(self, trained_models_list) -> TestItem: Tests the trained models. Parameters: trained_models_list : List of tuples containing sensor names, trained models, and test loaders. Returns: A list of tuples, each containing a sensor name, test predictions, test labels, and test metrics. run_pipeline def run_pipeline(self): Runs the entire pipeline, executing all steps from data download to model testing. Returns: A list of tuples containing test metrics for each sensor. Usage Example from phd_package.pipeline.pipeline_generator import Pipeline # Create a pipeline instance pipeline = Pipeline(experiment_name=\"my_experiment\") # Run the entire pipeline test_metrics = pipeline.run_pipeline() # Or run individual steps sensors_df = pipeline.read_or_download_sensors() raw_dfs = pipeline.read_or_download_data() preprocessed_dfs = pipeline.preprocess_data(raw_dfs) # ... and so on This Pipeline class provides a comprehensive workflow for processing sensor data, from raw data acquisition to model training and testing. Each method in the pipeline can be used independently or as part of the full run_pipeline process. Pipeline Helper Module The Pipeline class is supported by a set of helper functions that manage data processing, step execution, and configuration handling. These functions provide the underlying functionality for the pipeline's flexibility and modularity. Key areas supported by helper functions include: Configuration management for processing steps Dynamic execution of data processing steps Caching and loading of processed data Application of configurable processing steps to DataFrames For developers interested in extending or customising the pipeline, detailed documentation of these helper functions is available in the pipeline_helper.py module. This Pipeline class, along with its supporting helper functions, provides a comprehensive workflow for processing sensor data, from raw data acquisition to model training and testing. Each method in the pipeline can be used independently or as part of the full run_pipeline process.","title":"Pipeline Generator"},{"location":"pipeline_module/pipeline/#pipeline-generator","text":"The Pipeline class is the core component of the data processing pipeline. It orchestrates the entire process from downloading sensor data to training and testing models. The module uses 'mlflow` for experiment tracking.","title":"Pipeline Generator"},{"location":"pipeline_module/pipeline/#class-pipeline","text":"","title":"Class: Pipeline"},{"location":"pipeline_module/pipeline/#initialisation","text":"class Pipeline: def __init__(self, experiment_name: str = None): # Initialise the pipeline with an optional experiment name If no experiment_name is provided, a random string will be generated. The outputs from the model training and analysis are stored in the mlruns directory and can be viewed by calling mlflow ui in the command line.","title":"Initialisation"},{"location":"pipeline_module/pipeline/#methods","text":"","title":"Methods"},{"location":"pipeline_module/pipeline/#read_or_download_sensors","text":"def read_or_download_sensors(self) -> SensorListItem: Downloads the list of sensors from the API or reads from local storage if available. Returns: A DataFrame containing the sensor list.","title":"read_or_download_sensors"},{"location":"pipeline_module/pipeline/#read_or_download_data","text":"def read_or_download_data(self) -> RawDataItem: Reads raw sensor data from local storage if available or downloads it from the API. Returns: A list of tuples, each containing a sensor name and its raw data DataFrame.","title":"read_or_download_data"},{"location":"pipeline_module/pipeline/#preprocess_data","text":"def preprocess_data(self, raw_dfs) -> PreprocessedItem: Preprocesses the raw data. Parameters: raw_dfs : List of tuples containing sensor names and raw data DataFrames. Returns: A list of tuples, each containing a sensor name and its preprocessed data DataFrame.","title":"preprocess_data"},{"location":"pipeline_module/pipeline/#apply_feature_engineering","text":"def apply_feature_engineering(self, preprocessed_dfs) -> EngineeredItem: Applies feature engineering to preprocessed data. Parameters: preprocessed_dfs : List of tuples containing sensor names and preprocessed data DataFrames. Returns: A list of tuples, each containing a sensor name and its feature-engineered data DataFrame.","title":"apply_feature_engineering"},{"location":"pipeline_module/pipeline/#load_dataloader","text":"def load_dataloader(self, engineered_dfs) -> DataLoaderItem: Loads data into dataloaders . Parameters: engineered_dfs : List of tuples containing sensor names and feature-engineered data DataFrames. Returns: A list of tuples, each containing a sensor name, model, train loader, validation loader, and test loader.","title":"load_dataloader"},{"location":"pipeline_module/pipeline/#train_model","text":"def train_model(self, dataloaders_list) -> TrainedModelItem: Trains models using the prepared dataloaders . Parameters: dataloaders_list : List of tuples containing sensor names, models, and dataloaders . Returns: A list of tuples, each containing a sensor name, trained model, test loader, training metrics, and validation metrics.","title":"train_model"},{"location":"pipeline_module/pipeline/#test_model","text":"def test_model(self, trained_models_list) -> TestItem: Tests the trained models. Parameters: trained_models_list : List of tuples containing sensor names, trained models, and test loaders. Returns: A list of tuples, each containing a sensor name, test predictions, test labels, and test metrics.","title":"test_model"},{"location":"pipeline_module/pipeline/#run_pipeline","text":"def run_pipeline(self): Runs the entire pipeline, executing all steps from data download to model testing. Returns: A list of tuples containing test metrics for each sensor.","title":"run_pipeline"},{"location":"pipeline_module/pipeline/#usage-example","text":"from phd_package.pipeline.pipeline_generator import Pipeline # Create a pipeline instance pipeline = Pipeline(experiment_name=\"my_experiment\") # Run the entire pipeline test_metrics = pipeline.run_pipeline() # Or run individual steps sensors_df = pipeline.read_or_download_sensors() raw_dfs = pipeline.read_or_download_data() preprocessed_dfs = pipeline.preprocess_data(raw_dfs) # ... and so on This Pipeline class provides a comprehensive workflow for processing sensor data, from raw data acquisition to model training and testing. Each method in the pipeline can be used independently or as part of the full run_pipeline process.","title":"Usage Example"},{"location":"pipeline_module/pipeline/#pipeline-helper-module","text":"The Pipeline class is supported by a set of helper functions that manage data processing, step execution, and configuration handling. These functions provide the underlying functionality for the pipeline's flexibility and modularity. Key areas supported by helper functions include: Configuration management for processing steps Dynamic execution of data processing steps Caching and loading of processed data Application of configurable processing steps to DataFrames For developers interested in extending or customising the pipeline, detailed documentation of these helper functions is available in the pipeline_helper.py module. This Pipeline class, along with its supporting helper functions, provides a comprehensive workflow for processing sensor data, from raw data acquisition to model training and testing. Each method in the pipeline can be used independently or as part of the full run_pipeline process.","title":"Pipeline Helper Module"},{"location":"pipeline_module/stages/","text":"Pipeline Stages Documentation Each stage is designed so that the inputs and are immutable (cannot be changed). To achieve this a custom data is created for each stage. Data Types Before diving into the pipeline stages, let's look the key data types used throughout the pipeline: SensorListItem: TypeAlias = pd.DataFrame A pandas DataFrame containing information about sensors. RawDataItem: TypeAlias = list[Tuple[str, pd.DataFrame]] A list of tuples, each containing a sensor name (string) and its raw data (DataFrame). PreprocessedItem: TypeAlias = list[Tuple[str, pd.DataFrame]] A list of tuples, each containing a sensor name (string) and its preprocessed data (DataFrame). EngineeredItem: TypeAlias = list[Tuple[str, pd.DataFrame]] A list of tuples, each containing a sensor name (string) and its feature-engineered data (DataFrame). DataLoaderItem: TypeAlias = list[Tuple[str, nn.Module, DataLoader, DataLoader, DataLoader]] A list of tuples, each containing: Sensor name (string) Neural network model (nn.Module) Train DataLoader Validation DataLoader Test DataLoader TrainedModelItem: TypeAlias = list[Tuple[str, nn.Module, DataLoader, List, List]] A list of tuples, each containing: Sensor name (string) Trained neural network model (nn.Module) Test DataLoader List of training performance metrics List of validation performance metrics TestItem: TypeAlias = list[Tuple[str, np.ndarray, np.ndarray, dict]] A list of tuples, each containing: Sensor name (string) Test predictions (numpy array) Test labels (numpy array) Dictionary of test metrics 1. Preprocessing Stage The preprocessing stage prepares the raw data for further analysis. The purpose of this stage is to get rid of any fields that will not be used further down the pipeline. It also serves as an initial check to make sure all of the data is the correct format and datatypes. The steps below can optionally be configured to run. For most time series models, consecutive data windows are required, therefore running get_consecutive_sequences is highly recommended. Methods: initialise_preprocessing_pipeline(df: pd.DataFrame) -> pd.DataFrame Initialises the preprocessing pipeline by checking the DataFrame for required columns. Ensures 'Timestamp' and 'Value' columns are present and correctly formatted. remove_directionality_feature(df: pd.DataFrame, **kwargs) -> pd.DataFrame Removes directionality in data by aggregating values with the same timestamp. Useful for datasets where 'value' depends on a directional parameter. aggregate_on_datetime(df: pd.DataFrame, **kwargs) -> pd.DataFrame Aggregates data based on the datetime index, grouping by a specified frequency (default: 15min). Handles gaps in data by preserving original timestamps for large gaps. get_consecutive_sequences(df: pd.DataFrame) -> pd.DataFrame Finds all consecutive sequences in the input DataFrame longer than the specified window size. Assigns sequence numbers to each sequence. remove_specified_fields(df: pd.DataFrame, **kwargs) -> pd.DataFrame Removes specified columns from the DataFrame. Default columns to drop: [\"Time_Difference\", \"Interval_Minutes\"]. terminate_preprocessing_pipeline(df: pd.DataFrame) -> pd.DataFrame Performs final checks on the preprocessed DataFrame. Ensures all required columns are present and correctly formatted. 2. Feature Engineering Stage This stage creates new features and transforms existing ones. Similarly, to preprocessing, the following functions can be optionally configured to run. It is recommended that at least scaling is run. Methods: initialise_engineering_pipeline(df: pd.DataFrame) -> pd.DataFrame Initialises the feature engineering pipeline by checking the DataFrame for required columns. Ensures 'Timestamp' and 'Value' columns are present and correctly formatted. create_periodicity_features(df: pd.DataFrame) -> pd.DataFrame Generates frequency features for time series data. Creates sine and cosine features based on daily, half-day, weekly, quarterly, and yearly periods. datetime_to_float(df: pd.DataFrame) -> pd.DataFrame Converts the datetime column to a float representation. scale_features(df: pd.DataFrame, **kwargs) -> pd.DataFrame Scales all values in the DataFrame using StandardScaler. drop_datetime_column(df: pd.DataFrame) -> pd.DataFrame Drops the datetime column from the DataFrame. drop_sequence_column(df: pd.DataFrame) -> pd.DataFrame Drops the sequence column from the DataFrame. terminate_engineering_pipeline(df: pd.DataFrame) -> pd.DataFrame Performs final checks on the feature-engineered DataFrame. Ensures all required columns are present and correctly formatted. 3. Dataloader Stage This stage prepares the data for model training. The data is split into windows and added to a pytorch DataLoader object. Methods: sliding_windows(df: pd.DataFrame, **kwargs) -> Tuple[torch.tensor, torch.tensor] Generates sliding windows from the provided time-series data for sequence learning. Uses default values for training: window_size: 4 horizon 8 stride 1 create_dataloader(pipeline: Tuple[torch.Tensor, torch.Tensor, int], **kwargs) -> Tuple[DataLoader, DataLoader, DataLoader, int] Prepares training, validation, and test dataloaders . Uses default values to create the dataloaders : batch_size 64 shuffle: False num_workers: 0 train_ratio: 0.7 val_ratio: 0.15 add_model_to_dataloader(pipeline: Tuple[DataLoader, DataLoader, DataLoader, int], **kwargs) -> Tuple[torch.nn.Module, DataLoader, DataLoader, DataLoader] Adds the appropriate model (LSTM in this case) to the dataloader pipeline. 4. Model Training Stage This stage handles the training of the model. Methods: train_model(pipeline: DataLoaderItem, **kwargs) -> TrainedModelItem Trains and evaluates the neural network model. Uses default values for model training: epochs: 5 , optimiser: adam lr: 0.01 criterion: mse device: mps Implements a learning rate scheduler with: step_size: 0.01 gamma 0.1 Logs metrics using the mlflow module. 5. Model Testing Stage This stage evaluates the trained model. From the training stage, only the test DataLoader is passed into this stage. Methods: test_model(pipeline: List[TrainedModelItem], **kwargs) -> TestItem Evaluates the trained model on the test dataset. Calculates and returns test predictions, labels, and metrics. Each of these stages and their respective methods work together to process the data, engineer features, prepare it for modelling, train the model, and evaluate its performance. The entire process is customisable through the pipeline.json configuration file, allowing for flexibility in data processing and model training without changing the core code.","title":"Pipeline Stages"},{"location":"pipeline_module/stages/#pipeline-stages-documentation","text":"Each stage is designed so that the inputs and are immutable (cannot be changed). To achieve this a custom data is created for each stage.","title":"Pipeline Stages Documentation"},{"location":"pipeline_module/stages/#data-types","text":"Before diving into the pipeline stages, let's look the key data types used throughout the pipeline: SensorListItem: TypeAlias = pd.DataFrame A pandas DataFrame containing information about sensors. RawDataItem: TypeAlias = list[Tuple[str, pd.DataFrame]] A list of tuples, each containing a sensor name (string) and its raw data (DataFrame). PreprocessedItem: TypeAlias = list[Tuple[str, pd.DataFrame]] A list of tuples, each containing a sensor name (string) and its preprocessed data (DataFrame). EngineeredItem: TypeAlias = list[Tuple[str, pd.DataFrame]] A list of tuples, each containing a sensor name (string) and its feature-engineered data (DataFrame). DataLoaderItem: TypeAlias = list[Tuple[str, nn.Module, DataLoader, DataLoader, DataLoader]] A list of tuples, each containing: Sensor name (string) Neural network model (nn.Module) Train DataLoader Validation DataLoader Test DataLoader TrainedModelItem: TypeAlias = list[Tuple[str, nn.Module, DataLoader, List, List]] A list of tuples, each containing: Sensor name (string) Trained neural network model (nn.Module) Test DataLoader List of training performance metrics List of validation performance metrics TestItem: TypeAlias = list[Tuple[str, np.ndarray, np.ndarray, dict]] A list of tuples, each containing: Sensor name (string) Test predictions (numpy array) Test labels (numpy array) Dictionary of test metrics","title":"Data Types"},{"location":"pipeline_module/stages/#1-preprocessing-stage","text":"The preprocessing stage prepares the raw data for further analysis. The purpose of this stage is to get rid of any fields that will not be used further down the pipeline. It also serves as an initial check to make sure all of the data is the correct format and datatypes. The steps below can optionally be configured to run. For most time series models, consecutive data windows are required, therefore running get_consecutive_sequences is highly recommended. Methods:","title":"1. Preprocessing Stage"},{"location":"pipeline_module/stages/#initialise_preprocessing_pipelinedf-pddataframe-pddataframe","text":"Initialises the preprocessing pipeline by checking the DataFrame for required columns. Ensures 'Timestamp' and 'Value' columns are present and correctly formatted.","title":"initialise_preprocessing_pipeline(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#remove_directionality_featuredf-pddataframe-kwargs-pddataframe","text":"Removes directionality in data by aggregating values with the same timestamp. Useful for datasets where 'value' depends on a directional parameter.","title":"remove_directionality_feature(df: pd.DataFrame, **kwargs) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#aggregate_on_datetimedf-pddataframe-kwargs-pddataframe","text":"Aggregates data based on the datetime index, grouping by a specified frequency (default: 15min). Handles gaps in data by preserving original timestamps for large gaps.","title":"aggregate_on_datetime(df: pd.DataFrame, **kwargs) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#get_consecutive_sequencesdf-pddataframe-pddataframe","text":"Finds all consecutive sequences in the input DataFrame longer than the specified window size. Assigns sequence numbers to each sequence.","title":"get_consecutive_sequences(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#remove_specified_fieldsdf-pddataframe-kwargs-pddataframe","text":"Removes specified columns from the DataFrame. Default columns to drop: [\"Time_Difference\", \"Interval_Minutes\"].","title":"remove_specified_fields(df: pd.DataFrame, **kwargs) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#terminate_preprocessing_pipelinedf-pddataframe-pddataframe","text":"Performs final checks on the preprocessed DataFrame. Ensures all required columns are present and correctly formatted.","title":"terminate_preprocessing_pipeline(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#2-feature-engineering-stage","text":"This stage creates new features and transforms existing ones. Similarly, to preprocessing, the following functions can be optionally configured to run. It is recommended that at least scaling is run. Methods:","title":"2. Feature Engineering Stage"},{"location":"pipeline_module/stages/#initialise_engineering_pipelinedf-pddataframe-pddataframe","text":"Initialises the feature engineering pipeline by checking the DataFrame for required columns. Ensures 'Timestamp' and 'Value' columns are present and correctly formatted.","title":"initialise_engineering_pipeline(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#create_periodicity_featuresdf-pddataframe-pddataframe","text":"Generates frequency features for time series data. Creates sine and cosine features based on daily, half-day, weekly, quarterly, and yearly periods.","title":"create_periodicity_features(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#datetime_to_floatdf-pddataframe-pddataframe","text":"Converts the datetime column to a float representation.","title":"datetime_to_float(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#scale_featuresdf-pddataframe-kwargs-pddataframe","text":"Scales all values in the DataFrame using StandardScaler.","title":"scale_features(df: pd.DataFrame, **kwargs) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#drop_datetime_columndf-pddataframe-pddataframe","text":"Drops the datetime column from the DataFrame.","title":"drop_datetime_column(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#drop_sequence_columndf-pddataframe-pddataframe","text":"Drops the sequence column from the DataFrame.","title":"drop_sequence_column(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#terminate_engineering_pipelinedf-pddataframe-pddataframe","text":"Performs final checks on the feature-engineered DataFrame. Ensures all required columns are present and correctly formatted.","title":"terminate_engineering_pipeline(df: pd.DataFrame) -&gt; pd.DataFrame"},{"location":"pipeline_module/stages/#3-dataloader-stage","text":"This stage prepares the data for model training. The data is split into windows and added to a pytorch DataLoader object. Methods:","title":"3. Dataloader Stage"},{"location":"pipeline_module/stages/#sliding_windowsdf-pddataframe-kwargs-tupletorchtensor-torchtensor","text":"Generates sliding windows from the provided time-series data for sequence learning. Uses default values for training: window_size: 4 horizon 8 stride 1","title":"sliding_windows(df: pd.DataFrame, **kwargs) -&gt; Tuple[torch.tensor, torch.tensor]"},{"location":"pipeline_module/stages/#create_dataloaderpipeline-tupletorchtensor-torchtensor-int-kwargs-tupledataloader-dataloader-dataloader-int","text":"Prepares training, validation, and test dataloaders . Uses default values to create the dataloaders : batch_size 64 shuffle: False num_workers: 0 train_ratio: 0.7 val_ratio: 0.15","title":"create_dataloader(pipeline: Tuple[torch.Tensor, torch.Tensor, int], **kwargs) -&gt; Tuple[DataLoader, DataLoader, DataLoader, int]"},{"location":"pipeline_module/stages/#add_model_to_dataloaderpipeline-tupledataloader-dataloader-dataloader-int-kwargs-tupletorchnnmodule-dataloader-dataloader-dataloader","text":"Adds the appropriate model (LSTM in this case) to the dataloader pipeline.","title":"add_model_to_dataloader(pipeline: Tuple[DataLoader, DataLoader, DataLoader, int], **kwargs) -&gt; Tuple[torch.nn.Module, DataLoader, DataLoader, DataLoader]"},{"location":"pipeline_module/stages/#4-model-training-stage","text":"This stage handles the training of the model. Methods:","title":"4. Model Training Stage"},{"location":"pipeline_module/stages/#train_modelpipeline-dataloaderitem-kwargs-trainedmodelitem","text":"Trains and evaluates the neural network model. Uses default values for model training: epochs: 5 , optimiser: adam lr: 0.01 criterion: mse device: mps Implements a learning rate scheduler with: step_size: 0.01 gamma 0.1 Logs metrics using the mlflow module.","title":"train_model(pipeline: DataLoaderItem, **kwargs) -&gt; TrainedModelItem"},{"location":"pipeline_module/stages/#5-model-testing-stage","text":"This stage evaluates the trained model. From the training stage, only the test DataLoader is passed into this stage. Methods:","title":"5. Model Testing Stage"},{"location":"pipeline_module/stages/#test_modelpipeline-listtrainedmodelitem-kwargs-testitem","text":"Evaluates the trained model on the test dataset. Calculates and returns test predictions, labels, and metrics. Each of these stages and their respective methods work together to process the data, engineer features, prepare it for modelling, train the model, and evaluate its performance. The entire process is customisable through the pipeline.json configuration file, allowing for flexibility in data processing and model training without changing the core code.","title":"test_model(pipeline: List[TrainedModelItem], **kwargs) -&gt; TestItem"},{"location":"tutorials/overview/","text":"Overview This library is controlled through a series of config files. These files change how the pipeline preprocesses, engineers, trains and evaluates, as well as which types of data is queried in the first place. The main config files that will need to changed by the user are: query.json pipeline.json These two files control which data is selected for from the Urban Observatory database as an input for the pipeline, and how that data is then used in the pipeline. The query.json might look like this: { \"coords\": [ -1.611096, 54.968919, -1.607040, 54.972681 ], \"query_date_format\": \"startend\", \"theme\": \"People\", \"last_n_days\": 720, \"starttime\": 20220724, \"endtime\": 20240821 } The pipeline.json is more lengthy. It first defines the parameters that are used as input to define its behaviour. It then defines the steps that are executed when the pipeline is ran. New functions can be written by the user and added to the library during each of the processing stages, so long as they follow the I/O rules for each stage. The function can the be added as a step in the relevant stage below and will be executed in the pipeline. This is especially useful during the feature engineering stage, where functions that add various features might need to be experimented with and turned on and off. { \"kwargs\": { \"features_to_include_on_aggregation\": null, \"aggregation_frequency_mins\": \"15min\", \"columns_to_drop\": [ \"Time_Difference\", \"Interval_Minutes\" ], \"completeness_threshold\": 1.0, \"datetime_column\": \"Timestamp\", \"value_column\": \"Value\", \"min_df_length\": 10000, \"min_df_length_to_window_size_ratio\": 50, \"frequency\": null, \"scaler\": \"standard\", \"input_feature_indices\": null, \"target_feature_index\": 0, \"model_type\": \"lstm\", \"window_size\": 12, \"horizon\": 24, \"stride\": 1, \"batch_size\": 128, \"shuffle\": false, \"num_workers\": 0, \"train_ratio\": 0.7, \"val_ratio\": 0.15, \"device\": \"mps\", \"epochs\": 10, \"hidden_dim\": 128, \"num_layers\": 3, \"dropout\": 0.045499097918902325, \"optimiser\": \"adam\", \"lr\": 1.1965425636602132e-05, \"criterion\": \"mse\", \"momentum\": 0.9, \"scheduler_step_size\": 4, \"scheduler_gamma\": 0.4338623980132082, \"early_stopping_patience\": 3 }, \"preprocessing\": [ { \"name\": \"phd_package.pipeline.stages.preprocessing.initialise_preprocessing_pipeline\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.remove_directionality_feature\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.aggregate_on_datetime\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.get_consecutive_sequences\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.remove_specified_fields\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.terminate_preprocessing_pipeline\", \"execute_step\": true } ], \"feature_engineering\": [ { \"name\": \"phd_package.pipeline.stages.feature_engineering.initialise_engineering_pipeline\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.add_term_dates_feature\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.create_periodicity_features\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.extract_time_features\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.datetime_to_float\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.scale_features\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.resample_frequency\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.drop_datetime_column\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.drop_sequence_column\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.terminate_engineering_pipeline\", \"execute_step\": true } ], \"dataloader\": [ { \"name\": \"phd_package.pipeline.stages.dataloader.sliding_windows\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.dataloader.create_dataloader\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.dataloader.add_model_to_dataloader\", \"execute_step\": true } ], \"training\": [ { \"name\": \"phd_package.pipeline.stages.train_model.train_model\", \"execute_step\": true } ], \"testing\": [ { \"name\": \"phd_package.pipeline.stages.test_model.test_model\", \"execute_step\": true } ] }","title":"Overview"},{"location":"tutorials/overview/#overview","text":"This library is controlled through a series of config files. These files change how the pipeline preprocesses, engineers, trains and evaluates, as well as which types of data is queried in the first place. The main config files that will need to changed by the user are: query.json pipeline.json These two files control which data is selected for from the Urban Observatory database as an input for the pipeline, and how that data is then used in the pipeline. The query.json might look like this: { \"coords\": [ -1.611096, 54.968919, -1.607040, 54.972681 ], \"query_date_format\": \"startend\", \"theme\": \"People\", \"last_n_days\": 720, \"starttime\": 20220724, \"endtime\": 20240821 } The pipeline.json is more lengthy. It first defines the parameters that are used as input to define its behaviour. It then defines the steps that are executed when the pipeline is ran. New functions can be written by the user and added to the library during each of the processing stages, so long as they follow the I/O rules for each stage. The function can the be added as a step in the relevant stage below and will be executed in the pipeline. This is especially useful during the feature engineering stage, where functions that add various features might need to be experimented with and turned on and off. { \"kwargs\": { \"features_to_include_on_aggregation\": null, \"aggregation_frequency_mins\": \"15min\", \"columns_to_drop\": [ \"Time_Difference\", \"Interval_Minutes\" ], \"completeness_threshold\": 1.0, \"datetime_column\": \"Timestamp\", \"value_column\": \"Value\", \"min_df_length\": 10000, \"min_df_length_to_window_size_ratio\": 50, \"frequency\": null, \"scaler\": \"standard\", \"input_feature_indices\": null, \"target_feature_index\": 0, \"model_type\": \"lstm\", \"window_size\": 12, \"horizon\": 24, \"stride\": 1, \"batch_size\": 128, \"shuffle\": false, \"num_workers\": 0, \"train_ratio\": 0.7, \"val_ratio\": 0.15, \"device\": \"mps\", \"epochs\": 10, \"hidden_dim\": 128, \"num_layers\": 3, \"dropout\": 0.045499097918902325, \"optimiser\": \"adam\", \"lr\": 1.1965425636602132e-05, \"criterion\": \"mse\", \"momentum\": 0.9, \"scheduler_step_size\": 4, \"scheduler_gamma\": 0.4338623980132082, \"early_stopping_patience\": 3 }, \"preprocessing\": [ { \"name\": \"phd_package.pipeline.stages.preprocessing.initialise_preprocessing_pipeline\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.remove_directionality_feature\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.aggregate_on_datetime\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.get_consecutive_sequences\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.remove_specified_fields\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.preprocessing.terminate_preprocessing_pipeline\", \"execute_step\": true } ], \"feature_engineering\": [ { \"name\": \"phd_package.pipeline.stages.feature_engineering.initialise_engineering_pipeline\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.add_term_dates_feature\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.create_periodicity_features\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.extract_time_features\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.datetime_to_float\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.scale_features\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.resample_frequency\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.drop_datetime_column\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.drop_sequence_column\", \"execute_step\": false }, { \"name\": \"phd_package.pipeline.stages.feature_engineering.terminate_engineering_pipeline\", \"execute_step\": true } ], \"dataloader\": [ { \"name\": \"phd_package.pipeline.stages.dataloader.sliding_windows\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.dataloader.create_dataloader\", \"execute_step\": true }, { \"name\": \"phd_package.pipeline.stages.dataloader.add_model_to_dataloader\", \"execute_step\": true } ], \"training\": [ { \"name\": \"phd_package.pipeline.stages.train_model.train_model\", \"execute_step\": true } ], \"testing\": [ { \"name\": \"phd_package.pipeline.stages.test_model.test_model\", \"execute_step\": true } ] }","title":"Overview"},{"location":"tutorials/running_pipeline/","text":"Tutorial: Running the Pipeline To run the pipeline the following code can be executed in either of the following ways: Command Line The pipeline can be run as a module from the interpreter. This module requires poetry so should be ran using the following code: poetry run python -m phd_package.pipeline Python Interpreter >>>from phd_package.pipeline.pipeline_generator import Pipeline >>>pipeline = Pipeline() >>>pipeline.run_pipeline()","title":"Running Pipeline"},{"location":"tutorials/running_pipeline/#tutorial-running-the-pipeline","text":"To run the pipeline the following code can be executed in either of the following ways:","title":"Tutorial: Running the Pipeline"},{"location":"tutorials/running_pipeline/#command-line","text":"The pipeline can be run as a module from the interpreter. This module requires poetry so should be ran using the following code: poetry run python -m phd_package.pipeline","title":"Command Line"},{"location":"tutorials/running_pipeline/#python-interpreter","text":">>>from phd_package.pipeline.pipeline_generator import Pipeline >>>pipeline = Pipeline() >>>pipeline.run_pipeline()","title":"Python Interpreter"},{"location":"tutorials/viewing_data/","text":"Tutorial: Viewing the Urban Observatory Data Config Files The api.json config file contains the endpoints for the API request and the query parameters to be include in the request it is advised that these are left as they are. The values of the query parameters are found in the config/query.json file, this file requires input from the user. The coords parameter defines the area which sensors will be requested from. The theme parameter defines the type of sensors that will be looked for (in the example below it will look for pedestrian sensors). The query_date_format defines which of the subsequent three parameters will be used. In the example below it is set to startend which means the query will return data between the dates defined in starttime and endtime formatted as %Y%m%d . Alternatively the query_date_format parameter may be set to last_n_days in which case the query will return data from the number of days defined in the last_n_days parameter (in the case of the example below, the last 720 days). Example query.json { \"coords\": [ -1.611096, 54.968919, -1.607040, 54.972681 ], \"theme\": \"People\", \"query_date_format\": \"startend\", \"last_n_days\": 720, \"starttime\": 20220724, \"endtime\": 20240821 } Requesting Data With these parameters one can make a request using the api_data_processor module. The following example shows how this may be achieved. List of Sensors >>>from phd_package.api.api_data_processor import APIDataProcessor >>>processor = APIDataProcessor() >>>sensor_list = processor.execute_sensors_request() >>>sensor_list.head(5) Raw ID Sensor Name ... Broker Name Location (WKT) 0 79884 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... ... People Counting API POINT (-1.6105 54.9721) 1 79883 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_NORTH_TO_... ... People Counting API POINT (-1.6105 54.9721) 2 79869 PER_PEOPLE_GREYSTTHEATRESOUTH_SOUTHWEST_TO_NOR... ... People Counting API POINT (-1.6108 54.9722) 3 79868 PER_PEOPLE_GREYSTTHEATRESOUTH_SOUTHROAD_TO_NOR... ... People Counting API POINT (-1.610675161 54.972252194) 4 79859 PER_PEOPLE_GREYSTTHEATRESOUTH_SOUTHEAST_TO_NOR... ... People Counting API POINT (-1.6106 54.9723) [5 rows x 9 columns] Raw Sensor Data >>>from phd_package.api.api_data_processor import APIDataProcessor >>>processor = APIDataProcessor() >>>raw_data = processor.execute_data_request() >>>type(raw_data), type(raw_data[0]), type(raw_data[0][0]), type(raw_data[0][1]) (<class 'list'>, <class 'tuple'>, <class 'str'>, <class 'pandas.core.frame.DataFrame'>) name_of_first_sensor_in_list = raw_data[0][0] data_from_first_sensor_in_list = raw_data[0][1] print(name_of_first_sensor_in_list) data_from_first_sensor_in_list.head(5) PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_NORTH Sensor Name Variable ... Value Timestamp 0 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 27.0 2022-09-02 09:15:00 1 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 26.0 2022-09-02 09:30:00 2 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 14.0 2022-09-02 09:45:00 3 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 14.0 2022-09-02 10:00:00 4 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 23.0 2022-09-02 10:15:00","title":"Viewing Data"},{"location":"tutorials/viewing_data/#tutorial-viewing-the-urban-observatory-data","text":"","title":"Tutorial: Viewing the Urban Observatory Data"},{"location":"tutorials/viewing_data/#config-files","text":"The api.json config file contains the endpoints for the API request and the query parameters to be include in the request it is advised that these are left as they are. The values of the query parameters are found in the config/query.json file, this file requires input from the user. The coords parameter defines the area which sensors will be requested from. The theme parameter defines the type of sensors that will be looked for (in the example below it will look for pedestrian sensors). The query_date_format defines which of the subsequent three parameters will be used. In the example below it is set to startend which means the query will return data between the dates defined in starttime and endtime formatted as %Y%m%d . Alternatively the query_date_format parameter may be set to last_n_days in which case the query will return data from the number of days defined in the last_n_days parameter (in the case of the example below, the last 720 days).","title":"Config Files"},{"location":"tutorials/viewing_data/#example-queryjson","text":"{ \"coords\": [ -1.611096, 54.968919, -1.607040, 54.972681 ], \"theme\": \"People\", \"query_date_format\": \"startend\", \"last_n_days\": 720, \"starttime\": 20220724, \"endtime\": 20240821 }","title":"Example query.json"},{"location":"tutorials/viewing_data/#requesting-data","text":"With these parameters one can make a request using the api_data_processor module. The following example shows how this may be achieved.","title":"Requesting Data"},{"location":"tutorials/viewing_data/#list-of-sensors","text":">>>from phd_package.api.api_data_processor import APIDataProcessor >>>processor = APIDataProcessor() >>>sensor_list = processor.execute_sensors_request() >>>sensor_list.head(5) Raw ID Sensor Name ... Broker Name Location (WKT) 0 79884 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... ... People Counting API POINT (-1.6105 54.9721) 1 79883 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_NORTH_TO_... ... People Counting API POINT (-1.6105 54.9721) 2 79869 PER_PEOPLE_GREYSTTHEATRESOUTH_SOUTHWEST_TO_NOR... ... People Counting API POINT (-1.6108 54.9722) 3 79868 PER_PEOPLE_GREYSTTHEATRESOUTH_SOUTHROAD_TO_NOR... ... People Counting API POINT (-1.610675161 54.972252194) 4 79859 PER_PEOPLE_GREYSTTHEATRESOUTH_SOUTHEAST_TO_NOR... ... People Counting API POINT (-1.6106 54.9723) [5 rows x 9 columns]","title":"List of Sensors"},{"location":"tutorials/viewing_data/#raw-sensor-data","text":">>>from phd_package.api.api_data_processor import APIDataProcessor >>>processor = APIDataProcessor() >>>raw_data = processor.execute_data_request() >>>type(raw_data), type(raw_data[0]), type(raw_data[0][0]), type(raw_data[0][1]) (<class 'list'>, <class 'tuple'>, <class 'str'>, <class 'pandas.core.frame.DataFrame'>) name_of_first_sensor_in_list = raw_data[0][0] data_from_first_sensor_in_list = raw_data[0][1] print(name_of_first_sensor_in_list) data_from_first_sensor_in_list.head(5) PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_NORTH Sensor Name Variable ... Value Timestamp 0 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 27.0 2022-09-02 09:15:00 1 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 26.0 2022-09-02 09:30:00 2 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 14.0 2022-09-02 09:45:00 3 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 14.0 2022-09-02 10:00:00 4 PER_PEOPLE_NCLPILGRIMSTMARKETLN_FROM_SOUTH_TO_... Walking ... 23.0 2022-09-02 10:15:00","title":"Raw Sensor Data"}]}