{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Homepage","title":"Home"},{"location":"#homepage","text":"","title":"Homepage"},{"location":"api_module/__main__/","text":"main .py The __main__.py script serves as the entry point for the api module, facilitating the orchestration of API requests and data processing. Example Usage from api_client import ApiClient from api_data_processor import APIDataProcessor import json def main(): with open('api.json', 'r') as f: api_config = json.load(f) client = ApiClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Example query parameters query_params = { \"theme\": \"People\", \"polygon_wkb\": \"POLYGON((...))\" } # Fetch and process sensors data data_processor = APIDataProcessor() raw_sensors_data = client.get_sensors(query_params[\"theme\"], query_params[\"polygon_wkb\"]) sensors_df = data_processor.execute_sensors_request() # Process the data sensor_data = data_processor.execute_data_request() # Output the processed data print(sensor_data) if __name__ == \"__main__\": main()","title":"API Main"},{"location":"api_module/__main__/#mainpy","text":"The __main__.py script serves as the entry point for the api module, facilitating the orchestration of API requests and data processing.","title":"main.py"},{"location":"api_module/__main__/#example-usage","text":"from api_client import ApiClient from api_data_processor import APIDataProcessor import json def main(): with open('api.json', 'r') as f: api_config = json.load(f) client = ApiClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Example query parameters query_params = { \"theme\": \"People\", \"polygon_wkb\": \"POLYGON((...))\" } # Fetch and process sensors data data_processor = APIDataProcessor() raw_sensors_data = client.get_sensors(query_params[\"theme\"], query_params[\"polygon_wkb\"]) sensors_df = data_processor.execute_sensors_request() # Process the data sensor_data = data_processor.execute_data_request() # Output the processed data print(sensor_data) if __name__ == \"__main__\": main()","title":"Example Usage"},{"location":"api_module/api_client/","text":"Module: api_client.py The api_client.py script provides the core functionalities for making API requests and handling responses. It defines the APIClient class, which is used to interact with various endpoints of the API. Class: APIClient The APIClient class initialises with the base URL and token required for API access. It has several methods to interact with different endpoints defined in the api.json configuration file. Initialisation class APIClient: def __init__(self, base_url: str, token: str, endpoints: dict): self.base_url = base_url self.token = token self.endpoints = endpoints Methods get_sensors(theme: str, polygon_wkb: str) -> dict Fetches sensors based on the theme and polygon WKB (Well-Known Binary) format. Parameters : theme : The theme of the data (run APIClient.get_themes() for a full list). polygon_wkb : The polygon WKB defining the area of interest. Returns: JSON response containing sensor data. get_sensor_types() -> dict Fetches available sensor types. Returns: JSON response containing sensor types. get_themes() -> dict Fetches available themes. Returns: JSON response containing themes. get_variables(theme: str) -> dict Fetches variables based on the theme. Parameters: theme : The theme of the data (e.g., \u201cPeople\u201d). Returns: JSON response containing variables. get_data(sensor_name: str, starttime: str, endtime: str) -> dict Fetches data for a specific sensor within the given time range. Parameters: sensor_name : The name of the sensor. starttime : Start time for the data in YYYYMMDD format. endtime : End time for the data in YYYYMMDD format. Returns: JSON response containing sensor data. Example Usage from api_client import APIClient api_config = { \"base_url\": \"https://newcastle.urbanobservatory.ac.uk/api/v1.1/\", \"token\": \"your_api_token\", \"endpoints\": { # endpoint details } } client = APIClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Fetch sensors sensors = client.get_sensors(\"People\", \"POLYGON((...))\") # Fetch sensor types sensor_types = client.get_sensor_types() # Fetch themes themes = client.get_themes() # Fetch variables variables = client.get_variables(\"People\") # Fetch data data = client.get_data(\"sensor_name\", \"20220724\", \"20240724\")","title":"API Client"},{"location":"api_module/api_client/#module-api_clientpy","text":"The api_client.py script provides the core functionalities for making API requests and handling responses. It defines the APIClient class, which is used to interact with various endpoints of the API. Class: APIClient The APIClient class initialises with the base URL and token required for API access. It has several methods to interact with different endpoints defined in the api.json configuration file.","title":"Module: api_client.py"},{"location":"api_module/api_client/#initialisation","text":"class APIClient: def __init__(self, base_url: str, token: str, endpoints: dict): self.base_url = base_url self.token = token self.endpoints = endpoints","title":"Initialisation"},{"location":"api_module/api_client/#methods","text":"get_sensors(theme: str, polygon_wkb: str) -> dict Fetches sensors based on the theme and polygon WKB (Well-Known Binary) format. Parameters : theme : The theme of the data (run APIClient.get_themes() for a full list). polygon_wkb : The polygon WKB defining the area of interest. Returns: JSON response containing sensor data. get_sensor_types() -> dict Fetches available sensor types. Returns: JSON response containing sensor types. get_themes() -> dict Fetches available themes. Returns: JSON response containing themes. get_variables(theme: str) -> dict Fetches variables based on the theme. Parameters: theme : The theme of the data (e.g., \u201cPeople\u201d). Returns: JSON response containing variables. get_data(sensor_name: str, starttime: str, endtime: str) -> dict Fetches data for a specific sensor within the given time range. Parameters: sensor_name : The name of the sensor. starttime : Start time for the data in YYYYMMDD format. endtime : End time for the data in YYYYMMDD format. Returns: JSON response containing sensor data.","title":"Methods"},{"location":"api_module/api_client/#example-usage","text":"from api_client import APIClient api_config = { \"base_url\": \"https://newcastle.urbanobservatory.ac.uk/api/v1.1/\", \"token\": \"your_api_token\", \"endpoints\": { # endpoint details } } client = APIClient(api_config[\"base_url\"], api_config[\"token\"], api_config[\"endpoints\"]) # Fetch sensors sensors = client.get_sensors(\"People\", \"POLYGON((...))\") # Fetch sensor types sensor_types = client.get_sensor_types() # Fetch themes themes = client.get_themes() # Fetch variables variables = client.get_variables(\"People\") # Fetch data data = client.get_data(\"sensor_name\", \"20220724\", \"20240724\")","title":"Example Usage"},{"location":"api_module/api_data_processor/","text":"Module: api_data_processor.py The api_data_processor.py script handles the processing of data retrieved from the API. It defines the APIDataProcessor class, which is used to clean, preprocess, and manipulate the sensor data for further analysis. The APIDataProcessor requires a JSON config file with specified parameters. Class: APIDataProcessor The APIDataProcessor class initialises with configuration parameters and provides methods to execute various API requests and process the data. Initialisation class APIDataProcessor: def __init__(self): self.api_client = APIClient() self.config = get_api_config() self.window_size = get_window_size() self.min_df_length = get_min_df_length() self.ratio = get_min_df_length_to_window_size_ratio() Methods execute_sensors_request() -> pd.DataFrame Executes the API request to fetch sensor data and converts it to a DataFrame . Returns: DataFrame containing sensor data. execute_sensor_types_request() -> pd.DataFrame Executes the API request to fetch sensor types and converts it to a DataFrame . Returns: DataFrame containing sensor types. execute_themes_request() -> pd.DataFrame Executes the API request to fetch themes and converts it to a DataFrame . Returns: DataFrame containing themes. execute_variables_request() -> pd.DataFrame Executes the API request to fetch variables and converts it to a DataFrame . Returns: DataFrame containing variables. print_api_response_information(sensor_name: str, index: int, total_sensors: int) Prints information about the API response for a specific sensor. process_each_sensor(sensor_name: str, index: int, total_sensors: int) -> tuple Processes the data for a specific sensor. Parameters: sensor_name : The name of the sensor. index : The index of the sensor in the list. total_sensors : The total number of sensors. Returns: Tuple containing sensor name and DataFrame with sensor data. print_sensor_data_metrics(list_of_dataframes, series_of_sensor_names) Prints metrics about the processed sensor data. process_sensor_data_parallel(sensor_names: pd.Series) -> list Processes the data for sensors in parallel. Parameters: sensor_names : Series containing the names of the sensors. Returns: List of tuples containing sensor names and DataFrames with sensor data. execute_data_request() -> list Executes the data request and processes the data. Returns: List of tuples containing sensor names and DataFrames with sensor data. Example Usage from api_data_processor import APIDataProcessor data_processor = APIDataProcessor() # Execute sensor request and get the DataFrame sensors_df = data_processor.execute_sensors_request() # Execute sensor types request and get the DataFrame sensor_types_df = data_processor.execute_sensor_types_request() # Execute themes request and get the DataFrame themes_df = data_processor.execute_themes_request() # Execute variables request and get the DataFrame variables_df = data_processor.execute_variables_request() # Process sensor data sensor_data = data_processor.execute_data_request()","title":"API Data Processor"},{"location":"api_module/api_data_processor/#module-api_data_processorpy","text":"The api_data_processor.py script handles the processing of data retrieved from the API. It defines the APIDataProcessor class, which is used to clean, preprocess, and manipulate the sensor data for further analysis. The APIDataProcessor requires a JSON config file with specified parameters. Class: APIDataProcessor The APIDataProcessor class initialises with configuration parameters and provides methods to execute various API requests and process the data.","title":"Module: api_data_processor.py"},{"location":"api_module/api_data_processor/#initialisation","text":"class APIDataProcessor: def __init__(self): self.api_client = APIClient() self.config = get_api_config() self.window_size = get_window_size() self.min_df_length = get_min_df_length() self.ratio = get_min_df_length_to_window_size_ratio()","title":"Initialisation"},{"location":"api_module/api_data_processor/#methods","text":"execute_sensors_request() -> pd.DataFrame Executes the API request to fetch sensor data and converts it to a DataFrame . Returns: DataFrame containing sensor data. execute_sensor_types_request() -> pd.DataFrame Executes the API request to fetch sensor types and converts it to a DataFrame . Returns: DataFrame containing sensor types. execute_themes_request() -> pd.DataFrame Executes the API request to fetch themes and converts it to a DataFrame . Returns: DataFrame containing themes. execute_variables_request() -> pd.DataFrame Executes the API request to fetch variables and converts it to a DataFrame . Returns: DataFrame containing variables. print_api_response_information(sensor_name: str, index: int, total_sensors: int) Prints information about the API response for a specific sensor. process_each_sensor(sensor_name: str, index: int, total_sensors: int) -> tuple Processes the data for a specific sensor. Parameters: sensor_name : The name of the sensor. index : The index of the sensor in the list. total_sensors : The total number of sensors. Returns: Tuple containing sensor name and DataFrame with sensor data. print_sensor_data_metrics(list_of_dataframes, series_of_sensor_names) Prints metrics about the processed sensor data. process_sensor_data_parallel(sensor_names: pd.Series) -> list Processes the data for sensors in parallel. Parameters: sensor_names : Series containing the names of the sensors. Returns: List of tuples containing sensor names and DataFrames with sensor data. execute_data_request() -> list Executes the data request and processes the data. Returns: List of tuples containing sensor names and DataFrames with sensor data.","title":"Methods"},{"location":"api_module/api_data_processor/#example-usage","text":"from api_data_processor import APIDataProcessor data_processor = APIDataProcessor() # Execute sensor request and get the DataFrame sensors_df = data_processor.execute_sensors_request() # Execute sensor types request and get the DataFrame sensor_types_df = data_processor.execute_sensor_types_request() # Execute themes request and get the DataFrame themes_df = data_processor.execute_themes_request() # Execute variables request and get the DataFrame variables_df = data_processor.execute_variables_request() # Process sensor data sensor_data = data_processor.execute_data_request()","title":"Example Usage"},{"location":"api_module/overview/","text":"API Module Documentation This documentation provides an overview and usage details for the api module, which consists of functionalities to interact with the Newcastle Urban Observatory API. The module contains two main scripts: api_client.py and api_data_processor.py .","title":"API Module Overview"},{"location":"api_module/overview/#api-module-documentation","text":"This documentation provides an overview and usage details for the api module, which consists of functionalities to interact with the Newcastle Urban Observatory API. The module contains two main scripts: api_client.py and api_data_processor.py .","title":"API Module Documentation"},{"location":"pipeline_module/overview/","text":"PHD Package Pipeline Module Documentation Pipeline Module Overview The pipeline module is a core component of the PHD package, designed to handle the end-to-end process of time series data analysis and prediction. It encompasses several stages, from data loading to model training and evaluation. Key Components: Pipeline Generator : Orchestrates the entire data processing and modeling pipeline. Preprocessing : Includes functions for data cleaning and initial transformations. Feature Engineering : Provides methods for creating and transforming features. Dataloader : Handles the creation of PyTorch DataLoaders for model training. Model Training : Implements the training loop for neural network models. Model Testing : Evaluates the trained models on test data. Key Submodules 1. Pipeline Generator ( pipeline_generator.py ) The core submodule that orchestrates the entire data processing and modeling pipeline. It includes methods for each stage of the process, from data acquisition to model testing. 2. Preprocessing ( preprocessing.py ) Contains functions for data cleaning and initial transformations, such as: * Removing directionality features * Aggregating data on datetime * Finding consecutive sequences * Removing specified fields 3. Feature Engineering ( feature_engineering.py ) Provides methods for creating and transforming features, including: * Scaling features * Resampling frequency * Adding term dates features * Creating periodicity features * Converting datetime to float 4. Dataloader ( dataloader.py ) Handles the creation of PyTorch DataLoaders for model training. Key functions include: * sliding_windows : Generates sliding windows from time series data. * create_dataloader : Prepares training, validation, and test dataloaders. * add_model_to_dataloader : Adds the appropriate model to the dataloader pipeline. 5. Model Training ( train_model.py ) Implements the training loop for neural network models. The main function train_model handles: * Model training over specified epochs * Performance metric tracking * Validation during training 6. Model Testing ( test_model.py ) Evaluates the trained models on test data. Key functions include: * evaluate_model : Calculates performance metrics on a given dataset. * test_model : Evaluates the trained model on the test dataset. Usage The pipeline module is designed to be used as part of the larger PHD package. It's typically initiated through the main function, which creates an instance of the Pipeline class and calls its run_pipeline() method. Each stage of the pipeline can also be used independently for more granular control over the data processing and modeling workflow. Key Features Modular design allowing for easy customization and extension Integration with PyTorch for deep learning model implementation Comprehensive approach to time series analysis, from data preprocessing to model evaluation Utilization of MLflow for experiment tracking and metric logging Support for various types of models, including LSTM and autoencoder architectures This pipeline module provides a robust framework for time series analysis and prediction, encapsulating best practices in data preprocessing, feature engineering, and model training for time series data.","title":"Pipeline Module Overview"},{"location":"pipeline_module/overview/#phd-package-pipeline-module-documentation","text":"","title":"PHD Package Pipeline Module Documentation"},{"location":"pipeline_module/overview/#pipeline-module-overview","text":"The pipeline module is a core component of the PHD package, designed to handle the end-to-end process of time series data analysis and prediction. It encompasses several stages, from data loading to model training and evaluation.","title":"Pipeline Module Overview"},{"location":"pipeline_module/overview/#key-components","text":"Pipeline Generator : Orchestrates the entire data processing and modeling pipeline. Preprocessing : Includes functions for data cleaning and initial transformations. Feature Engineering : Provides methods for creating and transforming features. Dataloader : Handles the creation of PyTorch DataLoaders for model training. Model Training : Implements the training loop for neural network models. Model Testing : Evaluates the trained models on test data.","title":"Key Components:"},{"location":"pipeline_module/overview/#key-submodules","text":"","title":"Key Submodules"},{"location":"pipeline_module/overview/#1-pipeline-generator-pipeline_generatorpy","text":"The core submodule that orchestrates the entire data processing and modeling pipeline. It includes methods for each stage of the process, from data acquisition to model testing.","title":"1. Pipeline Generator (pipeline_generator.py)"},{"location":"pipeline_module/overview/#2-preprocessing-preprocessingpy","text":"Contains functions for data cleaning and initial transformations, such as: * Removing directionality features * Aggregating data on datetime * Finding consecutive sequences * Removing specified fields","title":"2. Preprocessing (preprocessing.py)"},{"location":"pipeline_module/overview/#3-feature-engineering-feature_engineeringpy","text":"Provides methods for creating and transforming features, including: * Scaling features * Resampling frequency * Adding term dates features * Creating periodicity features * Converting datetime to float","title":"3. Feature Engineering (feature_engineering.py)"},{"location":"pipeline_module/overview/#4-dataloader-dataloaderpy","text":"Handles the creation of PyTorch DataLoaders for model training. Key functions include: * sliding_windows : Generates sliding windows from time series data. * create_dataloader : Prepares training, validation, and test dataloaders. * add_model_to_dataloader : Adds the appropriate model to the dataloader pipeline.","title":"4. Dataloader (dataloader.py)"},{"location":"pipeline_module/overview/#5-model-training-train_modelpy","text":"Implements the training loop for neural network models. The main function train_model handles: * Model training over specified epochs * Performance metric tracking * Validation during training","title":"5. Model Training (train_model.py)"},{"location":"pipeline_module/overview/#6-model-testing-test_modelpy","text":"Evaluates the trained models on test data. Key functions include: * evaluate_model : Calculates performance metrics on a given dataset. * test_model : Evaluates the trained model on the test dataset.","title":"6. Model Testing (test_model.py)"},{"location":"pipeline_module/overview/#usage","text":"The pipeline module is designed to be used as part of the larger PHD package. It's typically initiated through the main function, which creates an instance of the Pipeline class and calls its run_pipeline() method. Each stage of the pipeline can also be used independently for more granular control over the data processing and modeling workflow.","title":"Usage"},{"location":"pipeline_module/overview/#key-features","text":"Modular design allowing for easy customization and extension Integration with PyTorch for deep learning model implementation Comprehensive approach to time series analysis, from data preprocessing to model evaluation Utilization of MLflow for experiment tracking and metric logging Support for various types of models, including LSTM and autoencoder architectures This pipeline module provides a robust framework for time series analysis and prediction, encapsulating best practices in data preprocessing, feature engineering, and model training for time series data.","title":"Key Features"},{"location":"pipeline_module/pipeline/","text":"Pipeline Generator The Pipeline class is the core component of the data processing pipeline. It orchestrates the entire process from downloading sensor data to training and testing models. The module uses 'mlflow` for experiment tracking. Class: Pipeline Initialisation class Pipeline: def __init__(self, experiment_name: str = None): # Initialize the pipeline with an optional experiment name If no experiment_name is provided, a random string will be generated. Methods read_or_download_sensors def read_or_download_sensors(self) -> SensorListItem: Downloads the list of sensors from the API or reads from local storage if available. Returns: A DataFrame containing the sensor list. read_or_download_data def read_or_download_data(self) -> RawDataItem: Reads raw sensor data from local storage if available or downloads it from the API. Returns: A list of tuples, each containing a sensor name and its raw data DataFrame. preprocess_data def preprocess_data(self, raw_dfs) -> PreprocessedItem: Preprocesses the raw data. Parameters: - raw_dfs : List of tuples containing sensor names and raw data DataFrames. Returns: A list of tuples, each containing a sensor name and its preprocessed data DataFrame. apply_feature_engineering def apply_feature_engineering(self, preprocessed_dfs) -> EngineeredItem: Applies feature engineering to preprocessed data. Parameters: - preprocessed_dfs : List of tuples containing sensor names and preprocessed data DataFrames. Returns: A list of tuples, each containing a sensor name and its feature-engineered data DataFrame. load_dataloader def load_dataloader(self, engineered_dfs) -> DataLoaderItem: Loads data into dataloaders. Parameters: - engineered_dfs : List of tuples containing sensor names and feature-engineered data DataFrames. Returns: A list of tuples, each containing a sensor name, model, train loader, validation loader, and test loader. train_model def train_model(self, dataloaders_list) -> TrainedModelItem: Trains models using the prepared dataloaders. Parameters: - dataloaders_list : List of tuples containing sensor names, models, and dataloaders. Returns: A list of tuples, each containing a sensor name, trained model, test loader, training metrics, and validation metrics. test_model def test_model(self, trained_models_list) -> TestItem: Tests the trained models. Parameters: - trained_models_list : List of tuples containing sensor names, trained models, and test loaders. Returns: A list of tuples, each containing a sensor name, test predictions, test labels, and test metrics. run_pipeline def run_pipeline(self): Runs the entire pipeline, executing all steps from data download to model testing. Returns: A list of tuples containing test metrics for each sensor. Usage Example from phd_package.pipeline.pipeline_generator import Pipeline # Create a pipeline instance pipeline = Pipeline(experiment_name=\"my_experiment\") # Run the entire pipeline test_metrics = pipeline.run_pipeline() # Or run individual steps sensors_df = pipeline.read_or_download_sensors() raw_dfs = pipeline.read_or_download_data() preprocessed_dfs = pipeline.preprocess_data(raw_dfs) # ... and so on This Pipeline class provides a comprehensive workflow for processing sensor data, from raw data acquisition to model training and testing. Each method in the pipeline can be used independently or as part of the full run_pipeline process.","title":"Pipeline Generator"},{"location":"pipeline_module/pipeline/#pipeline-generator","text":"The Pipeline class is the core component of the data processing pipeline. It orchestrates the entire process from downloading sensor data to training and testing models. The module uses 'mlflow` for experiment tracking.","title":"Pipeline Generator"},{"location":"pipeline_module/pipeline/#class-pipeline","text":"","title":"Class: Pipeline"},{"location":"pipeline_module/pipeline/#initialisation","text":"class Pipeline: def __init__(self, experiment_name: str = None): # Initialize the pipeline with an optional experiment name If no experiment_name is provided, a random string will be generated.","title":"Initialisation"},{"location":"pipeline_module/pipeline/#methods","text":"","title":"Methods"},{"location":"pipeline_module/pipeline/#read_or_download_sensors","text":"def read_or_download_sensors(self) -> SensorListItem: Downloads the list of sensors from the API or reads from local storage if available. Returns: A DataFrame containing the sensor list.","title":"read_or_download_sensors"},{"location":"pipeline_module/pipeline/#read_or_download_data","text":"def read_or_download_data(self) -> RawDataItem: Reads raw sensor data from local storage if available or downloads it from the API. Returns: A list of tuples, each containing a sensor name and its raw data DataFrame.","title":"read_or_download_data"},{"location":"pipeline_module/pipeline/#preprocess_data","text":"def preprocess_data(self, raw_dfs) -> PreprocessedItem: Preprocesses the raw data. Parameters: - raw_dfs : List of tuples containing sensor names and raw data DataFrames. Returns: A list of tuples, each containing a sensor name and its preprocessed data DataFrame.","title":"preprocess_data"},{"location":"pipeline_module/pipeline/#apply_feature_engineering","text":"def apply_feature_engineering(self, preprocessed_dfs) -> EngineeredItem: Applies feature engineering to preprocessed data. Parameters: - preprocessed_dfs : List of tuples containing sensor names and preprocessed data DataFrames. Returns: A list of tuples, each containing a sensor name and its feature-engineered data DataFrame.","title":"apply_feature_engineering"},{"location":"pipeline_module/pipeline/#load_dataloader","text":"def load_dataloader(self, engineered_dfs) -> DataLoaderItem: Loads data into dataloaders. Parameters: - engineered_dfs : List of tuples containing sensor names and feature-engineered data DataFrames. Returns: A list of tuples, each containing a sensor name, model, train loader, validation loader, and test loader.","title":"load_dataloader"},{"location":"pipeline_module/pipeline/#train_model","text":"def train_model(self, dataloaders_list) -> TrainedModelItem: Trains models using the prepared dataloaders. Parameters: - dataloaders_list : List of tuples containing sensor names, models, and dataloaders. Returns: A list of tuples, each containing a sensor name, trained model, test loader, training metrics, and validation metrics.","title":"train_model"},{"location":"pipeline_module/pipeline/#test_model","text":"def test_model(self, trained_models_list) -> TestItem: Tests the trained models. Parameters: - trained_models_list : List of tuples containing sensor names, trained models, and test loaders. Returns: A list of tuples, each containing a sensor name, test predictions, test labels, and test metrics.","title":"test_model"},{"location":"pipeline_module/pipeline/#run_pipeline","text":"def run_pipeline(self): Runs the entire pipeline, executing all steps from data download to model testing. Returns: A list of tuples containing test metrics for each sensor.","title":"run_pipeline"},{"location":"pipeline_module/pipeline/#usage-example","text":"from phd_package.pipeline.pipeline_generator import Pipeline # Create a pipeline instance pipeline = Pipeline(experiment_name=\"my_experiment\") # Run the entire pipeline test_metrics = pipeline.run_pipeline() # Or run individual steps sensors_df = pipeline.read_or_download_sensors() raw_dfs = pipeline.read_or_download_data() preprocessed_dfs = pipeline.preprocess_data(raw_dfs) # ... and so on This Pipeline class provides a comprehensive workflow for processing sensor data, from raw data acquisition to model training and testing. Each method in the pipeline can be used independently or as part of the full run_pipeline process.","title":"Usage Example"}]}